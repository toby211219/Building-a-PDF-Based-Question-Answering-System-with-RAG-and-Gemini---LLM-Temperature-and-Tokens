{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K_jsPkGGRq1k",
        "outputId": "a181e783-0ebe-46d3-c507-e8b710019d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Task 3: Temperature Experiment ---\n",
            "\n",
            "--- Testing Temperature: 0.0 ---\n",
            "Prompt: Based on general knowledge, explain how generative AI models learn and create content?\n",
            "Answer (0.0):\n",
            "Generative AI models learn and create content through a process that broadly involves three key steps: **training, learning, and generation**.\n",
            "\n",
            "**1. Training:**  This is where the model acquires its knowledge.  It's fed massive amounts of data – text, images, audio, code, etc., depending on the type of content it's designed to generate.  This data is often pre-processed and cleaned to improve the model's learning.  The specific architecture of the model (e.g., a large language model like GPT-3, an image generation model like DALL-E 2) dictates how this data is used.\n",
            "\n",
            "**2. Learning:**  The core of the learning process involves identifying patterns and relationships within the training data.  This is achieved through complex algorithms, most commonly variations of neural networks.  These networks consist of interconnected nodes (neurons) organized in layers.  During training, the model adjusts the strength of the connections between these neurons\n",
            "\n",
            "--- Testing Temperature: 0.2 ---\n",
            "Prompt: Based on general knowledge, explain how generative AI models learn and create content?\n",
            "Answer (0.2):\n",
            "Generative AI models, like large language models (LLMs) and image generators, learn and create content through a process that broadly involves three key steps:\n",
            "\n",
            "1. **Training on Massive Datasets:**  These models are trained on enormous datasets of text, images, audio, or code, depending on their purpose.  For example, an LLM might be trained on a massive corpus of books, articles, code, and websites.  This dataset provides the raw material the model learns from.  The training process involves feeding this data to the model and adjusting its internal parameters (weights and biases) to minimize the difference between its predictions and the actual data.  This is essentially a statistical process of finding patterns and relationships within the data.\n",
            "\n",
            "2. **Learning Statistical Relationships:** The core of the learning process is identifying statistical relationships within the training data.  The model doesn't \"understand\" the data in the human sense; instead, it learns to predict the probability of one element (word\n",
            "\n",
            "--- Testing Temperature: 0.5 ---\n",
            "Prompt: Based on general knowledge, explain how generative AI models learn and create content?\n",
            "Answer (0.5):\n",
            "Generative AI models learn and create content through a process that broadly involves three key stages: **training, learning, and generation.**\n",
            "\n",
            "**1. Training:**  This stage involves feeding the model massive amounts of data.  This data could be text (for text generation), images (for image generation), audio (for music or speech generation), or a combination thereof. The data needs to be representative of the type of content the model is intended to generate.  For example, a model trained to write poems will be fed a vast corpus of poems.\n",
            "\n",
            "The data is not simply memorized. Instead, the model uses algorithms, primarily variations of neural networks (especially deep learning models like transformers), to identify patterns, relationships, and statistical regularities within the data.  Think of it like learning the grammar and style of a language by reading countless books and articles – but on a vastly larger scale and with far more sophisticated algorithms.\n",
            "\n",
            "**2. Learning:** During the learning phase, the model adjusts\n",
            "\n",
            "--- Testing Temperature: 0.8 ---\n",
            "Prompt: Based on general knowledge, explain how generative AI models learn and create content?\n",
            "Answer (0.8):\n",
            "Generative AI models learn and create content through a process that broadly involves three key stages: **training, learning, and generation.**\n",
            "\n",
            "**1. Training:**  This is where the model acquires its knowledge.  It's fed massive amounts of data – text, images, audio, code, etc., depending on the model's purpose. This data is often pre-processed and cleaned to improve the quality of learning.  For example, a text-based model might be trained on a vast corpus of books, articles, and websites.\n",
            "\n",
            "**2. Learning:** This is where the magic happens.  The model doesn't \"understand\" the data in the human sense, but it learns statistical relationships and patterns within the data.  Different models use different techniques:\n",
            "\n",
            "* **Neural Networks (especially deep learning):**  These models consist of interconnected nodes (neurons) organized in layers.  During training, the model adjusts the strengths of the connections between these neurons to better represent\n",
            "\n",
            "--- Testing Temperature: 1.0 ---\n",
            "Prompt: Based on general knowledge, explain how generative AI models learn and create content?\n",
            "Answer (1.0):\n",
            "Generative AI models learn and create content through a process that broadly involves three main steps: **training, learning, and generation.**\n",
            "\n",
            "**1. Training:**\n",
            "\n",
            "This stage involves feeding the model massive amounts of data.  This data can be text (for text generation), images (for image generation), audio (for music or speech generation), or a combination thereof.  The data needs to be relevant to the type of content the AI is intended to generate. For example, a model trained to write poems will be fed a large corpus of poetry.\n",
            "\n",
            "The data is not simply stored; it's processed and analyzed to identify patterns, relationships, and statistical probabilities between different elements. This is often done using a technique called **deep learning**, specifically employing neural networks with multiple layers (hence \"deep\").  These networks have billions of parameters (weights and biases) that are adjusted during the training process.\n",
            "\n",
            "**2. Learning:**\n",
            "\n",
            "The learning process is essentially the adjustment of those parameters.\n",
            "\n",
            "--- Temperature Experiment Ended ---\n",
            "\n",
            "Experiment Results Summary:\n",
            "\n",
            "Temperature 0.0:\n",
            "Generative AI models learn and create content through a process that broadly involves three key steps: **training, learning, and generation**.\n",
            "\n",
            "**1. Training:**  This is where the model acquires its knowledge.  It's fed massive amounts of data – text, images, audio, code, etc., depending on the type of content it's designed to generate.  This data is often pre-processed and cleaned to improve the model's learning.  The specific architecture of the model (e.g., a large language model like GPT-3, an image generation model like DALL-E 2) dictates how this data is used.\n",
            "\n",
            "**2. Learning:**  The core of the learning process involves identifying patterns and relationships within the training data.  This is achieved through complex algorithms, most commonly variations of neural networks.  These networks consist of interconnected nodes (neurons) organized in layers.  During training, the model adjusts the strength of the connections between these neurons\n",
            "------------------------------\n",
            "\n",
            "Temperature 0.2:\n",
            "Generative AI models, like large language models (LLMs) and image generators, learn and create content through a process that broadly involves three key steps:\n",
            "\n",
            "1. **Training on Massive Datasets:**  These models are trained on enormous datasets of text, images, audio, or code, depending on their purpose.  For example, an LLM might be trained on a massive corpus of books, articles, code, and websites.  This dataset provides the raw material the model learns from.  The training process involves feeding this data to the model and adjusting its internal parameters (weights and biases) to minimize the difference between its predictions and the actual data.  This is essentially a statistical process of finding patterns and relationships within the data.\n",
            "\n",
            "2. **Learning Statistical Relationships:** The core of the learning process is identifying statistical relationships within the training data.  The model doesn't \"understand\" the data in the human sense; instead, it learns to predict the probability of one element (word\n",
            "------------------------------\n",
            "\n",
            "Temperature 0.5:\n",
            "Generative AI models learn and create content through a process that broadly involves three key stages: **training, learning, and generation.**\n",
            "\n",
            "**1. Training:**  This stage involves feeding the model massive amounts of data.  This data could be text (for text generation), images (for image generation), audio (for music or speech generation), or a combination thereof. The data needs to be representative of the type of content the model is intended to generate.  For example, a model trained to write poems will be fed a vast corpus of poems.\n",
            "\n",
            "The data is not simply memorized. Instead, the model uses algorithms, primarily variations of neural networks (especially deep learning models like transformers), to identify patterns, relationships, and statistical regularities within the data.  Think of it like learning the grammar and style of a language by reading countless books and articles – but on a vastly larger scale and with far more sophisticated algorithms.\n",
            "\n",
            "**2. Learning:** During the learning phase, the model adjusts\n",
            "------------------------------\n",
            "\n",
            "Temperature 0.8:\n",
            "Generative AI models learn and create content through a process that broadly involves three key stages: **training, learning, and generation.**\n",
            "\n",
            "**1. Training:**  This is where the model acquires its knowledge.  It's fed massive amounts of data – text, images, audio, code, etc., depending on the model's purpose. This data is often pre-processed and cleaned to improve the quality of learning.  For example, a text-based model might be trained on a vast corpus of books, articles, and websites.\n",
            "\n",
            "**2. Learning:** This is where the magic happens.  The model doesn't \"understand\" the data in the human sense, but it learns statistical relationships and patterns within the data.  Different models use different techniques:\n",
            "\n",
            "* **Neural Networks (especially deep learning):**  These models consist of interconnected nodes (neurons) organized in layers.  During training, the model adjusts the strengths of the connections between these neurons to better represent\n",
            "------------------------------\n",
            "\n",
            "Temperature 1.0:\n",
            "Generative AI models learn and create content through a process that broadly involves three main steps: **training, learning, and generation.**\n",
            "\n",
            "**1. Training:**\n",
            "\n",
            "This stage involves feeding the model massive amounts of data.  This data can be text (for text generation), images (for image generation), audio (for music or speech generation), or a combination thereof.  The data needs to be relevant to the type of content the AI is intended to generate. For example, a model trained to write poems will be fed a large corpus of poetry.\n",
            "\n",
            "The data is not simply stored; it's processed and analyzed to identify patterns, relationships, and statistical probabilities between different elements. This is often done using a technique called **deep learning**, specifically employing neural networks with multiple layers (hence \"deep\").  These networks have billions of parameters (weights and biases) that are adjusted during the training process.\n",
            "\n",
            "**2. Learning:**\n",
            "\n",
            "The learning process is essentially the adjustment of those parameters.\n",
            "------------------------------\n",
            "\n",
            "Please observe the answers at different temperatures above, consider their differences in randomness, creativity, and coherence, and record your findings in subsequent tasks.\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Assuming you have configured the API key\n",
        "# genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
        "# Or paste your key directly here\n",
        "genai.configure(api_key=\"AIzaSyC54VU8zneaNJ2hwyJW2jpW-lAl-9UfO28\")\n",
        "\n",
        "model = genai.GenerativeModel('models/gemini-1.5-flash-latest')\n",
        "\n",
        "def generate_answer(prompt: str, temperature: float = 0.5, max_output_tokens: int = 200) -> str:\n",
        "    generation_config = genai.GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        max_output_tokens=max_output_tokens\n",
        "    )\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            prompt,\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating content: {e}\")\n",
        "        return \"Unable to generate answer, please try again later.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Task 3: Temperature Experiment ---\")\n",
        "\n",
        "    # TODO: Replace with your actual RAG prompt, or a general question that simulates RAG\n",
        "    # This prompt should be the final string you pass to the generate_answer function,\n",
        "    # containing relevant content retrieved from the PDF and the user's question.\n",
        "    # Here we use a general question as an example.\n",
        "    rag_related_prompt = \"Based on general knowledge, explain how generative AI models learn and create content?\"\n",
        "\n",
        "    # Set the temperature values to experiment with\n",
        "    temperatures_to_test = [0.0, 0.2, 0.5, 0.8, 1.0]\n",
        "    fixed_max_tokens = 200 # Fix the maximum tokens in this experiment\n",
        "\n",
        "    # Dictionary to store experiment results\n",
        "    experiment_results = {}\n",
        "\n",
        "    for temp in temperatures_to_test:\n",
        "        print(f\"\\n--- Testing Temperature: {temp} ---\")\n",
        "        print(f\"Prompt: {rag_related_prompt}\")\n",
        "\n",
        "        # Call your generate_answer function with different temperatures\n",
        "        answer = generate_answer(rag_related_prompt, temperature=temp, max_output_tokens=fixed_max_tokens)\n",
        "\n",
        "        print(f\"Answer ({temp}):\\n{answer}\")\n",
        "\n",
        "        # Record the results\n",
        "        experiment_results[temp] = answer\n",
        "\n",
        "    print(\"\\n--- Temperature Experiment Ended ---\")\n",
        "    print(\"\\nExperiment Results Summary:\")\n",
        "    for temp, answer in experiment_results.items():\n",
        "        print(f\"\\nTemperature {temp}:\")\n",
        "        print(answer)\n",
        "        print(\"-\" * 30) # Separator line\n",
        "\n",
        "    print(\"\\nPlease observe the answers at different temperatures above, consider their differences in randomness, creativity, and coherence, and record your findings in subsequent tasks.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "1yUuluY4VQ41"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}