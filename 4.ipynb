{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K_jsPkGGRq1k",
        "outputId": "2ee060a6-7be2-4dc1-d54e-a85722dc23e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Task 4: Token Limit Experiment ---\n",
            "\n",
            "--- Testing Max Output Tokens: 20 ---\n",
            "Prompt: Please explain in detail the working principles, advantages, and challenges of RAG (Retrieval-Augmented Generation) systems, and illustrate their differences from traditional LLM applications.\n",
            "Answer (20 tokens):\n",
            "Retrieval-Augmented Generation (RAG) systems represent a significant advancement in large language model (LLM\n",
            "\n",
            "--- Testing Max Output Tokens: 50 ---\n",
            "Prompt: Please explain in detail the working principles, advantages, and challenges of RAG (Retrieval-Augmented Generation) systems, and illustrate their differences from traditional LLM applications.\n",
            "Answer (50 tokens):\n",
            "Retrieval-Augmented Generation (RAG) systems represent a significant advancement in large language model (LLM) applications. Unlike traditional LLMs that rely solely on their internal knowledge base, RAG systems augment the LLM's capabilities by connecting it to an\n",
            "\n",
            "--- Testing Max Output Tokens: 100 ---\n",
            "Prompt: Please explain in detail the working principles, advantages, and challenges of RAG (Retrieval-Augmented Generation) systems, and illustrate their differences from traditional LLM applications.\n",
            "Answer (100 tokens):\n",
            "Retrieval-Augmented Generation (RAG) systems represent a significant advancement in large language model (LLM) applications.  Unlike traditional LLMs that rely solely on their internal knowledge base (the parameters learned during training), RAG systems augment the LLM's capabilities by incorporating external knowledge sources at inference time. This allows them to access and process information beyond what was available during their training, resulting in more accurate, up-to-date, and contextually relevant responses.\n",
            "\n",
            "**Working Principles:**\n",
            "\n",
            "\n",
            "\n",
            "--- Testing Max Output Tokens: 200 ---\n",
            "Prompt: Please explain in detail the working principles, advantages, and challenges of RAG (Retrieval-Augmented Generation) systems, and illustrate their differences from traditional LLM applications.\n",
            "Answer (200 tokens):\n",
            "Retrieval-Augmented Generation (RAG) systems represent a significant advancement in large language model (LLM) applications.  Unlike traditional LLMs that rely solely on their internal knowledge base (the weights of the model itself), RAG systems augment the LLM's capabilities by retrieving relevant information from an external knowledge base before generating a response. This external knowledge base can be a variety of sources, including databases, documents, or even the web.\n",
            "\n",
            "**Working Principles:**\n",
            "\n",
            "1. **Retrieval:**  When a user provides a query or prompt, a retrieval module first searches the external knowledge base for relevant information.  This module employs techniques like keyword matching, semantic search (using embeddings and similarity measures like cosine similarity), or vector databases to find the most pertinent documents or passages.  The effectiveness of this step is crucial for the overall performance of the RAG system.\n",
            "\n",
            "2. **Contextualization:** The retrieved information is then contextualized and formatted to be suitable for the LLM.  This might\n",
            "\n",
            "--- Testing Max Output Tokens: 400 ---\n",
            "Prompt: Please explain in detail the working principles, advantages, and challenges of RAG (Retrieval-Augmented Generation) systems, and illustrate their differences from traditional LLM applications.\n",
            "Answer (400 tokens):\n",
            "Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional Large Language Models (LLMs) by enhancing their capabilities with external knowledge sources. Instead of relying solely on the knowledge embedded within their parameters during training, RAG systems retrieve relevant information from a knowledge base before generating a response. This allows them to handle more complex tasks, access up-to-date information, and provide more accurate and contextually relevant outputs.\n",
            "\n",
            "**Working Principles of RAG Systems:**\n",
            "\n",
            "A RAG system typically consists of three main components:\n",
            "\n",
            "1. **Retrieval Module:** This component is responsible for identifying relevant information from the knowledge base based on the user's query.  This involves several steps:\n",
            "    * **Query Embedding:** The user's query is transformed into a numerical vector representation (embedding) using techniques like Sentence-BERT or other embedding models.\n",
            "    * **Document Embedding:** Documents within the knowledge base are also converted into vector representations.\n",
            "    * **Similarity Search:** The query embedding is compared to the document embeddings using similarity metrics (e.g., cosine similarity) to find the most relevant documents.  Efficient search methods like FAISS or Annoy are often employed to handle large knowledge bases.\n",
            "    * **Ranking and Selection:**  The top-ranked documents are selected for further processing.  This stage might involve re-ranking based on other factors like document relevance scores or metadata.\n",
            "\n",
            "2. **Retrieval Context Generation:** The retrieved documents are then processed to create a concise and relevant context for the LLM. This might involve:\n",
            "    * **Extractive Summarization:** Selecting relevant sentences or paragraphs from the retrieved documents.\n",
            "    * **Passage Re-ranking:** Re-ranking the extracted passages based on their relevance to the query.\n",
            "    * **Context Fusion:** Combining multiple relevant passages into a coherent context.\n",
            "\n",
            "3. **Generation Module:** The LLM receives the user's query and the generated context from the retrieval module as input. It then uses\n",
            "\n",
            "--- Token Limit Experiment Ended ---\n",
            "\n",
            "Experiment Results Summary:\n",
            "\n",
            "Max Output Tokens 20:\n",
            "Retrieval-Augmented Generation (RAG) systems represent a significant advancement in large language model (LLM\n",
            "------------------------------\n",
            "\n",
            "Max Output Tokens 50:\n",
            "Retrieval-Augmented Generation (RAG) systems represent a significant advancement in large language model (LLM) applications. Unlike traditional LLMs that rely solely on their internal knowledge base, RAG systems augment the LLM's capabilities by connecting it to an\n",
            "------------------------------\n",
            "\n",
            "Max Output Tokens 100:\n",
            "Retrieval-Augmented Generation (RAG) systems represent a significant advancement in large language model (LLM) applications.  Unlike traditional LLMs that rely solely on their internal knowledge base (the parameters learned during training), RAG systems augment the LLM's capabilities by incorporating external knowledge sources at inference time. This allows them to access and process information beyond what was available during their training, resulting in more accurate, up-to-date, and contextually relevant responses.\n",
            "\n",
            "**Working Principles:**\n",
            "\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Max Output Tokens 200:\n",
            "Retrieval-Augmented Generation (RAG) systems represent a significant advancement in large language model (LLM) applications.  Unlike traditional LLMs that rely solely on their internal knowledge base (the weights of the model itself), RAG systems augment the LLM's capabilities by retrieving relevant information from an external knowledge base before generating a response. This external knowledge base can be a variety of sources, including databases, documents, or even the web.\n",
            "\n",
            "**Working Principles:**\n",
            "\n",
            "1. **Retrieval:**  When a user provides a query or prompt, a retrieval module first searches the external knowledge base for relevant information.  This module employs techniques like keyword matching, semantic search (using embeddings and similarity measures like cosine similarity), or vector databases to find the most pertinent documents or passages.  The effectiveness of this step is crucial for the overall performance of the RAG system.\n",
            "\n",
            "2. **Contextualization:** The retrieved information is then contextualized and formatted to be suitable for the LLM.  This might\n",
            "------------------------------\n",
            "\n",
            "Max Output Tokens 400:\n",
            "Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional Large Language Models (LLMs) by enhancing their capabilities with external knowledge sources. Instead of relying solely on the knowledge embedded within their parameters during training, RAG systems retrieve relevant information from a knowledge base before generating a response. This allows them to handle more complex tasks, access up-to-date information, and provide more accurate and contextually relevant outputs.\n",
            "\n",
            "**Working Principles of RAG Systems:**\n",
            "\n",
            "A RAG system typically consists of three main components:\n",
            "\n",
            "1. **Retrieval Module:** This component is responsible for identifying relevant information from the knowledge base based on the user's query.  This involves several steps:\n",
            "    * **Query Embedding:** The user's query is transformed into a numerical vector representation (embedding) using techniques like Sentence-BERT or other embedding models.\n",
            "    * **Document Embedding:** Documents within the knowledge base are also converted into vector representations.\n",
            "    * **Similarity Search:** The query embedding is compared to the document embeddings using similarity metrics (e.g., cosine similarity) to find the most relevant documents.  Efficient search methods like FAISS or Annoy are often employed to handle large knowledge bases.\n",
            "    * **Ranking and Selection:**  The top-ranked documents are selected for further processing.  This stage might involve re-ranking based on other factors like document relevance scores or metadata.\n",
            "\n",
            "2. **Retrieval Context Generation:** The retrieved documents are then processed to create a concise and relevant context for the LLM. This might involve:\n",
            "    * **Extractive Summarization:** Selecting relevant sentences or paragraphs from the retrieved documents.\n",
            "    * **Passage Re-ranking:** Re-ranking the extracted passages based on their relevance to the query.\n",
            "    * **Context Fusion:** Combining multiple relevant passages into a coherent context.\n",
            "\n",
            "3. **Generation Module:** The LLM receives the user's query and the generated context from the retrieval module as input. It then uses\n",
            "------------------------------\n",
            "Program execution finished.\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Ensure your API key is configured\n",
        "# genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
        "genai.configure(api_key=\"AIzaSyC54VU8zneaNJ2hwyJW2jpW-lAl-9UfO28\")\n",
        "\n",
        "model = genai.GenerativeModel('models/gemini-1.5-flash-latest')\n",
        "\n",
        "def generate_answer(prompt: str, temperature: float = 0.5, max_output_tokens: int = 200) -> str:\n",
        "    generation_config = genai.GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        max_output_tokens=max_output_tokens\n",
        "    )\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            prompt,\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating content: {e}\")\n",
        "        return \"Failed to generate answer, please try again later.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Task 4: Token Limit Experiment ---\")\n",
        "\n",
        "    # Choose a prompt that will produce a longer answer\n",
        "    # Replace with a complex prompt your RAG system might generate or use\n",
        "    long_answer_prompt = \"Please explain in detail the working principles, advantages, and challenges of RAG (Retrieval-Augmented Generation) systems, and illustrate their differences from traditional LLM applications.\"\n",
        "\n",
        "    # Set the maximum output tokens to experiment with\n",
        "    tokens_to_test = [20, 50, 100, 200, 400]\n",
        "    fixed_temperature = 0.5 # Fix temperature in this experiment\n",
        "\n",
        "    # Dictionary to store experiment results\n",
        "    experiment_results = {}\n",
        "\n",
        "    for tokens in tokens_to_test:\n",
        "        print(f\"\\n--- Testing Max Output Tokens: {tokens} ---\")\n",
        "        print(f\"Prompt: {long_answer_prompt}\")\n",
        "\n",
        "        # Call your generate_answer function, passing different maximum token counts\n",
        "        answer = generate_answer(long_answer_prompt, temperature=fixed_temperature, max_output_tokens=tokens)\n",
        "\n",
        "        print(f\"Answer ({tokens} tokens):\\n{answer}\")\n",
        "\n",
        "        # Record results\n",
        "        experiment_results[tokens] = answer\n",
        "\n",
        "    print(\"\\n--- Token Limit Experiment Ended ---\")\n",
        "    print(\"\\nExperiment Results Summary:\")\n",
        "    for tokens, answer in experiment_results.items():\n",
        "        print(f\"\\nMax Output Tokens {tokens}:\")\n",
        "        print(answer)\n",
        "        print(\"-\" * 30) # Separator\n",
        "\n",
        "print(\"Program execution finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "1yUuluY4VQ41"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}